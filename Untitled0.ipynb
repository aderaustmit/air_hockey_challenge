{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/aderaustmit/air_hockey_challenge/blob/warm-up/Untitled0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "oBfdVdpCNKxg"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(\"test\")\n",
    "\n",
    "import torch\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class Network(nn.Module):\n",
    "    def __init__(self, input_shape, output_shape, n_features, **kwargs):\n",
    "        super(Network, self).__init__()\n",
    "\n",
    "        n_input = input_shape[-1]\n",
    "        n_output = output_shape[0]\n",
    "\n",
    "        self._h1 = nn.Linear(n_input, n_features)\n",
    "        self._h2 = nn.Linear(n_features, n_features)\n",
    "        self._h3 = nn.Linear(n_features, n_output)\n",
    "\n",
    "        nn.init.xavier_uniform_(self._h1.weight,\n",
    "                                gain=nn.init.calculate_gain('tanh'))\n",
    "        nn.init.xavier_uniform_(self._h2.weight,\n",
    "                                gain=nn.init.calculate_gain('tanh'))\n",
    "        nn.init.xavier_uniform_(self._h3.weight,\n",
    "                                gain=nn.init.calculate_gain('linear'))\n",
    "\n",
    "    def forward(self, obs, **kwargs):\n",
    "        features1 = torch.tanh(self._h1(torch.squeeze(obs, 1).float()))\n",
    "        features2 = torch.tanh(self._h2(features1))\n",
    "        a = self._h3(features2)\n",
    "\n",
    "        return a\n",
    "    \n",
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, input_shape, output_shape, n_features, **kwargs):\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "        self.output_shape = output_shape\n",
    "\n",
    "        n_input = input_shape[-1]\n",
    "        n_output = output_shape[0]#*output_shape[1]\n",
    "\n",
    "        self._h1 = nn.Linear(n_input, n_features)\n",
    "        self._h2 = nn.Linear(n_features, n_features)\n",
    "        self._h3 = nn.Linear(n_features, n_output)\n",
    "\n",
    "        nn.init.xavier_uniform_(self._h1.weight,\n",
    "                                gain=nn.init.calculate_gain('tanh'))\n",
    "        nn.init.xavier_uniform_(self._h2.weight,\n",
    "                                gain=nn.init.calculate_gain('tanh'))\n",
    "        nn.init.xavier_uniform_(self._h3.weight,\n",
    "                                gain=nn.init.calculate_gain('linear'))\n",
    "\n",
    "    def forward(self, obs, **kwargs):\n",
    "        features1 = torch.tanh(self._h1(torch.squeeze(obs, 1).float()))\n",
    "        features2 = torch.tanh(self._h2(features1))\n",
    "        a = self._h3(features2)\n",
    "\n",
    "        return a#.reshape(self.output_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mushroom_rl.core import Core\n",
    "from air_hockey_agent.agent_builder import DummyAgent\n",
    "from air_hockey_challenge.utils.kinematics import forward_kinematics\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class TrainingCore(Core):\n",
    "    def __init__(self, *args, action_idx=None, **kwargs):\n",
    "        if action_idx:\n",
    "            self.action_idx = action_idx\n",
    "        else:\n",
    "            self.action_idx = [0, 1]\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "        self.dummy_agent = DummyAgent(self.mdp.env_info) # only used for inverse kinematics\n",
    "    \n",
    "    def _step(self, render):\n",
    "        \"\"\"\n",
    "        Single step.\n",
    "\n",
    "        Args:\n",
    "            render (bool):\n",
    "                whether to render or not.\n",
    "\n",
    "        Returns:\n",
    "            A tuple containing the previous state, the action sampled by the\n",
    "            agent, the reward obtained, the reached state, the absorbing flag\n",
    "            of the reached state and the last step flag.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        flat_action = self.agent.draw_action(self._state)\n",
    "        action = flat_action.reshape(2,3)\n",
    "\n",
    "        next_state, reward, absorbing, step_info = self.mdp.step(action[self.action_idx])\n",
    "        \n",
    "        self._episode_steps += 1\n",
    "\n",
    "        if render:\n",
    "            self.mdp.render()\n",
    "\n",
    "        last = self._episode_steps >= self.mdp.info.horizon or absorbing\n",
    "\n",
    "        state = self._state\n",
    "        next_state = self._preprocess(next_state.copy())\n",
    "        self._state = next_state\n",
    "\n",
    "        training_reward = self.reward_function(state, action, reward, next_state, absorbing, last)\n",
    "\n",
    "        step_info = {}\n",
    "\n",
    "        return (state, flat_action, training_reward, next_state, absorbing, last), step_info\n",
    "    \n",
    "    def reward_function(self, state, action, reward, next_state, absorbing, last):\n",
    "        total_reward = 0\n",
    "        ee_pos = forward_kinematics(self.dummy_agent.robot_model, self.dummy_agent.robot_data, self.dummy_agent.get_joint_pos(next_state))[0]\n",
    "        puck_pos = self.dummy_agent.get_puck_pos(next_state)\n",
    "        total_reward = -np.linalg.norm(ee_pos - puck_pos)\n",
    "        return total_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/04/2023 16:37:24 [INFO] ###################################################################################################\n",
      "20/04/2023 16:37:24 [INFO] Experiment Algorithm: PPO\n"
     ]
    }
   ],
   "source": [
    "from air_hockey_challenge.framework.air_hockey_challenge_wrapper import AirHockeyChallengeWrapper\n",
    "\n",
    "\n",
    "from mushroom_rl.core import Logger\n",
    "from mushroom_rl.algorithms.actor_critic import PPO\n",
    "from mushroom_rl.policy import GaussianTorchPolicy\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "cuda = torch.cuda.is_available()\n",
    "\n",
    "logger = Logger(PPO.__name__, results_dir=None)\n",
    "logger.strong_line()\n",
    "logger.info('Experiment Algorithm: ' + PPO.__name__)\n",
    "\n",
    "mdp = AirHockeyChallengeWrapper(\"3dof-hit\")\n",
    "mdp.reset()\n",
    "\n",
    "critic_params = dict(network=Network,\n",
    "                        optimizer={'class': torch.optim.Adam,\n",
    "                                'params': {}},\n",
    "                        loss=torch.nn.functional.mse_loss,\n",
    "                        n_features=64,\n",
    "                        batch_size=64,\n",
    "                        input_shape=(12,),\n",
    "                        output_shape=(1,), \n",
    "                        use_cuda = cuda)\n",
    "\n",
    "alg_params = dict(actor_optimizer={'class': torch.optim.Adam,\n",
    "                                    'params': {}},\n",
    "                    critic_params=critic_params,\n",
    "                    n_epochs_policy = 1,\n",
    "                    batch_size = 5,\n",
    "                    eps_ppo = 0.01,\n",
    "                    lam = 0.95,\n",
    "                 )\n",
    "\n",
    "policy_params = dict(\n",
    "    std_0=1.,\n",
    "    n_features=64,\n",
    "    use_cuda=cuda\n",
    ")\n",
    "\n",
    "policy = GaussianTorchPolicy(PolicyNetwork,\n",
    "                                (12,),\n",
    "                                (6,),\n",
    "                                **policy_params)\n",
    "policy.load(\"dataset/policy\")\n",
    "\n",
    "agent = PPO(mdp.info, policy, **alg_params)\n",
    "\n",
    "# Algorithm\n",
    "core = TrainingCore(agent, mdp)\n",
    "\n",
    "def train(n_epochs, n_steps, n_steps_per_fit):\n",
    "    core.learn(n_steps=n_steps, n_steps_per_fit=n_steps_per_fit)\n",
    "\n",
    "    # RUN\n",
    "    # dataset = core.evaluate(n_steps=n_step_test, render=False)\n",
    "    # avg_reward = \n",
    "    # logger.epoch_info(0, avg_reward)\n",
    "\n",
    "    for n in tqdm(range(n_epochs)):\n",
    "        core.learn(n_steps=n_steps, n_steps_per_fit=n_steps_per_fit)\n",
    "        # dataset = core.evaluate(n_steps=n_step_test, render=False)\n",
    "        # avg_reward = \n",
    "        # logger.epoch_info(n+1, avg_reward)\n",
    "\n",
    "    logger.info('Tranining end')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:56<00:00,  1.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/04/2023 16:39:04 [INFO] Tranining end\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train(n_epochs=100, n_steps=50, n_steps_per_fit=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "core.evaluate(n_episodes=5, render=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expert_trajs = generate_demonstration_data(expert_agent=expert_agent,\n",
    "                                           env=env,\n",
    "                                           num_trials=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class TrajDataset(Dataset):\n",
    "    def __init__(self, trajs):\n",
    "        states = []\n",
    "        actions = []\n",
    "        for traj in trajs:\n",
    "            states.append(traj.obs)\n",
    "            actions.append(traj.actions)\n",
    "        self.states = np.concatenate(states, axis=0)\n",
    "        self.actions = np.concatenate(actions, axis=0)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.states.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = dict()\n",
    "        sample['state'] = self.states[idx]\n",
    "        sample['action'] = self.actions[idx]\n",
    "        return sample\n",
    "    \n",
    "    def add_traj(self, traj=None, states=None, actions=None):\n",
    "        if traj is not None:\n",
    "            self.states = np.concatenate((self.states, traj.obs), axis=0)\n",
    "            self.actions = np.concatenate((self.actions, traj.actions), axis=0)\n",
    "        else:\n",
    "            self.states = np.concatenate((self.states, states), axis=0)\n",
    "            self.actions = np.concatenate((self.actions, actions), axis=0)\n",
    "\n",
    "def generate_demonstration_data(expert_agent, env, num_traj):\n",
    "    traj_dataset = TrajDataset()\n",
    "    runner = EpisodicRunner(agent=agent, env=env)\n",
    "    for trial_id in tqdm(range(num_trials), desc='Run'):\n",
    "        env.reset()\n",
    "        traj = runner()\n",
    "        trajs.append(traj)\n",
    "    return trajs\n",
    "\n",
    "\n",
    "def train_bc_agent(agent, trajs, max_epochs=5000, batch_size=256, lr=0.0005, disable_tqdm=True):\n",
    "    dataset = TrajDataset(trajs)\n",
    "    dataloader = DataLoader(dataset, \n",
    "                            batch_size=batch_size, \n",
    "                            shuffle=True)\n",
    "    optimizer = optim.Adam(agent.actor.parameters(),\n",
    "                           lr=lr)\n",
    "    pbar = tqdm(range(max_epochs), desc='Epoch', disable=disable_tqdm)\n",
    "    logs = dict(loss=[], epoch=[])\n",
    "    for iter in pbar:\n",
    "        avg_loss = []\n",
    "        for batch_idx, sample in enumerate(dataloader):\n",
    "            states = sample['state'].float().to(cfg.alg.device)\n",
    "            expert_actions = sample['action'].float().to(cfg.alg.device)\n",
    "            optimizer.zero_grad()\n",
    "            act_dist, _ = agent.actor(states)\n",
    "            #### TODO: compute the loss in a variable named as 'loss'\n",
    "            #### using the act_dist and expert_actions\n",
    "            loss = -act_dist.log_prob(expert_actions).mean()\n",
    "            ####\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            pbar.set_postfix({'loss': loss.item()})\n",
    "            avg_loss.append(loss.item())\n",
    "        logs['loss'].append(np.mean(avg_loss))\n",
    "        logs['epoch'].append(iter)\n",
    "    return agent, logs, len(dataset)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNrmP6hXOIoS2cW74xktEOX",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
