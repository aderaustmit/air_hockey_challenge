{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/aderaustmit/air_hockey_challenge/blob/warm-up/Untitled0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "oBfdVdpCNKxg"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(\"test\")\n",
    "\n",
    "import torch\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from air_hockey_challenge.framework.evaluate_agent import PENALTY_POINTS\n",
    "from air_hockey_challenge.utils.kinematics import forward_kinematics\n",
    "import numpy as np\n",
    "\n",
    "class Network(nn.Module):\n",
    "    def __init__(self, input_shape, output_shape, n_features, **kwargs):\n",
    "        super(Network, self).__init__()\n",
    "\n",
    "        self._h1 = nn.Linear(input_shape[0], n_features)\n",
    "        self._h2 = nn.Linear(n_features, n_features)\n",
    "        self._h3 = nn.Linear(n_features, output_shape[0])\n",
    "\n",
    "        nn.init.xavier_uniform_(self._h1.weight,\n",
    "                                gain=nn.init.calculate_gain('tanh'))\n",
    "        nn.init.xavier_uniform_(self._h2.weight,\n",
    "                                gain=nn.init.calculate_gain('tanh'))\n",
    "        nn.init.xavier_uniform_(self._h3.weight,\n",
    "                                gain=nn.init.calculate_gain('linear'))\n",
    "\n",
    "    def forward(self, obs, **kwargs):\n",
    "        features1 = torch.tanh(self._h1(torch.squeeze(obs, 1).float()))\n",
    "        features2 = torch.tanh(self._h2(features1))\n",
    "        return self._h3(features2)\n",
    "    \n",
    "def custom_reward_function(base_env, state, action, next_state, absorbing):\n",
    "    reward = -0.1 # small value to disincentivate long runs: faster is better\n",
    "\n",
    "    q = state[base_env.env_info['joint_pos_ids']]\n",
    "    dq = state[base_env.env_info['joint_vel_ids']]\n",
    "\n",
    "    # Get a dictionary of the constraint functions {\"constraint_name\": ndarray}\n",
    "    c = base_env.env_info['constraints'].fun(q, dq)\n",
    "\n",
    "    # for k,v in c.items():\n",
    "    #     reward -= PENALTY_POINTS[k] * np.sum(v > 0)\n",
    "    # mybe not ideal: penalty is flat outside, does not know how to improve --> use violation instead of points?\n",
    "\n",
    "    for k,val in c.items():\n",
    "        for v in val:\n",
    "            reward -= 100*v if v > 0 else 0\n",
    "\n",
    "    # TODO reward for good actions --> score\n",
    "    # for now --> try to go to the puck\n",
    "    \n",
    "    ee_pos = forward_kinematics(base_env.env_info['robot']['robot_model'], base_env.env_info['robot']['robot_data'], q)[0]\n",
    "    ee_pos = ee_pos - np.array([1.51,0,0.1])\n",
    "    puck_pos = state[base_env.env_info['puck_pos_ids']]\n",
    "    puck_vel = np.linalg.norm(state[base_env.env_info['puck_vel_ids']])\n",
    "    print(puck_vel)\n",
    "    reward += -np.linalg.norm(ee_pos - puck_pos) if puck_vel < 0.1 else puck_vel\n",
    "\n",
    "    return reward\n",
    "\n",
    "from mushroom_rl.algorithms.actor_critic import PPO\n",
    "\n",
    "class PPOAgent(PPO): # Just to fix the action shape\n",
    "    def draw_action(self, state):\n",
    "        return super().draw_action(state).reshape(2,3)\n",
    "    \n",
    "    def fit(self, dataset, **info):\n",
    "        for i in range(len(dataset)):\n",
    "            dataset[i] = list(dataset[i])\n",
    "            dataset[i][1] = dataset[i][1].flatten()\n",
    "        return super().fit(dataset, **info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from air_hockey_challenge.framework.air_hockey_challenge_wrapper import AirHockeyChallengeWrapper\n",
    "from air_hockey_challenge.framework.challenge_core import ChallengeCore\n",
    "\n",
    "\n",
    "from mushroom_rl.core import Logger\n",
    "from mushroom_rl.algorithms.actor_critic import PPO\n",
    "from mushroom_rl.policy import GaussianTorchPolicy\n",
    "\n",
    "from mushroom_rl.utils.dataset import compute_metrics\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "# use_cuda = False\n",
    "\n",
    "# logger = Logger(PPO.__name__, results_dir=None)\n",
    "# logger.strong_line()\n",
    "# logger.info('Experiment Algorithm: ' + PPO.__name__)\n",
    "\n",
    "mdp = AirHockeyChallengeWrapper(\"3dof-hit\", custom_reward_function=custom_reward_function)\n",
    "mdp.reset()\n",
    "\n",
    "critic_params = dict(network=Network,\n",
    "                        optimizer={'class': torch.optim.Adam,\n",
    "                                'params': {}},\n",
    "                        loss=torch.nn.functional.mse_loss,\n",
    "                        n_features=64,\n",
    "                        batch_size=64,\n",
    "                        input_shape=(12,),\n",
    "                        output_shape=(1,), \n",
    "                        use_cuda = use_cuda)\n",
    "\n",
    "alg_params = dict(actor_optimizer={'class': torch.optim.Adam,\n",
    "                                    'params': {}},\n",
    "                    critic_params=critic_params,\n",
    "                    n_epochs_policy = 5,\n",
    "                    batch_size = 32,\n",
    "                    eps_ppo = 0.01,\n",
    "                    lam = 0.99,\n",
    "                 )\n",
    "\n",
    "policy_params = dict(\n",
    "    std_0=0.01,\n",
    "    n_features=64,\n",
    "    use_cuda=use_cuda\n",
    ")\n",
    "\n",
    "policy = GaussianTorchPolicy(Network,\n",
    "                                (12,),\n",
    "                                (6,),\n",
    "                                **policy_params)\n",
    "\n",
    "policy.load(\"logs/bc_ppo/bc/policy.pkl\")\n",
    "\n",
    "agent = PPOAgent(mdp.info, policy, **alg_params)\n",
    "\n",
    "# Algorithm\n",
    "core = ChallengeCore(agent, mdp)\n",
    "\n",
    "log_metrics = []\n",
    "\n",
    "def train(n_epochs, n_episode_learn, n_episode_eval, n_episode_per_fit):\n",
    "    dataset = core.evaluate(n_episodes=n_episode_eval, render=False)\n",
    "    metrics = compute_metrics(dataset)\n",
    "    # logger.epoch_info(0, metrics=metrics)\n",
    "    # logger.info('Tranining start')\n",
    "    print(\"Epoch:\",0,\"metrics:\",metrics)\n",
    "    print(\"Training start\")\n",
    "    \n",
    "    for n in tqdm(range(n_epochs)):\n",
    "        core.learn(n_episodes=n_episode_learn, n_episodes_per_fit=n_episode_per_fit)\n",
    "        dataset = core.evaluate(n_episodes=n_episode_eval, render=False)\n",
    "        metrics = compute_metrics(dataset)\n",
    "        # logger.epoch_info(n+1, metrics=metrics)\n",
    "        print(\"Epoch:\",n+1,\"metrics:\",metrics)\n",
    "        log_metrics.append(metrics)\n",
    "        agent.save(f\"dataset/PPO_BC/new_epoch_{n+1}.pkl\")\n",
    "\n",
    "    # logger.info('Tranining end')\n",
    "    print(\"Training end\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(n_epochs=25, n_episode_learn=100, n_episode_eval=5, n_episode_per_fit=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdp.env_info[\"robot\"][\"base_frame\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                             \r"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "dataset = core.evaluate(n_episodes=5, render=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================================\n",
      "Environment:        3dof-hit\n",
      "Number of Episodes: 50\n",
      "Success:            0.0200\n",
      "Penalty:            226\n",
      "Number of Violations: \n",
      "  joint_vel_constr  50\n",
      "  ee_constr         42\n",
      "  Jerk              50\n",
      "  Total             142\n",
      "-------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from air_hockey_challenge.framework.evaluate_agent import evaluate\n",
    "from mushroom_rl.policy import GaussianTorchPolicy\n",
    "\n",
    "policy = GaussianTorchPolicy(Network,\n",
    "                                (12,),\n",
    "                                (6,),\n",
    "                                **policy_params)\n",
    "\n",
    "policy.load(\"dataset/PPO_BC/new_epoch_25.pkl\")\n",
    "\n",
    "def build_agent(env_info, **kwargs):\n",
    "    \"\"\"\n",
    "    Function where an Agent that controls the environments should be returned.\n",
    "    The Agent should inherit from the mushroom_rl Agent base env.\n",
    "\n",
    "    :param env_info: The environment information\n",
    "    :return: Either Agent ot Policy\n",
    "    \"\"\"\n",
    "    if \"hit\" in env_info[\"env_name\"]:\n",
    "        policy = GaussianTorchPolicy(Network,\n",
    "                                (12,),\n",
    "                                (6,),\n",
    "                                **policy_params)\n",
    "        policy.load(\"dataset/PPO2/epoch_10.pkl\")\n",
    "\n",
    "        agent = PPOAgent(mdp.info, policy, **alg_params)\n",
    "        return agent\n",
    "\n",
    "eval = evaluate(build_agent, 'logs/ppo_agent', env_list=[\"3dof-hit\"], n_episodes=50, render=False, quiet=False, n_cores=1)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNrmP6hXOIoS2cW74xktEOX",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
