{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cuda: True\n"
     ]
    }
   ],
   "source": [
    "from air_hockey_challenge.framework.air_hockey_challenge_wrapper import AirHockeyChallengeWrapper\n",
    "from air_hockey_challenge.framework.challenge_core import ChallengeCore\n",
    "from air_hockey_challenge.framework.agent_base import AgentBase\n",
    "from examples.control.hitting_agent import build_agent as build_agent_hitting\n",
    "\n",
    "from mushroom_rl.utils.dataset import parse_dataset, select_random_samples\n",
    "from mushroom_rl.policy import GaussianTorchPolicy\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = 'cuda' if use_cuda else 'cpu'\n",
    "print(f\"Cuda: {use_cuda}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self, input_shape, output_shape, n_features, **kwargs):\n",
    "        super(Network, self).__init__()\n",
    "\n",
    "        self._h1 = nn.Linear(input_shape[0], n_features)\n",
    "        self._h2 = nn.Linear(n_features, n_features)\n",
    "        self._h3 = nn.Linear(n_features, output_shape[0])\n",
    "\n",
    "        nn.init.xavier_uniform_(self._h1.weight,\n",
    "                                gain=nn.init.calculate_gain('tanh'))\n",
    "        nn.init.xavier_uniform_(self._h2.weight,\n",
    "                                gain=nn.init.calculate_gain('tanh'))\n",
    "        nn.init.xavier_uniform_(self._h3.weight,\n",
    "                                gain=nn.init.calculate_gain('linear'))\n",
    "\n",
    "    def forward(self, obs, **kwargs):\n",
    "        features1 = torch.tanh(self._h1(torch.squeeze(obs, 1).float()))\n",
    "        features2 = torch.tanh(self._h2(features1))\n",
    "        return self._h3(features2)\n",
    "\n",
    "# policy can only output 1D actions (6,)\n",
    "policy = GaussianTorchPolicy(Network, (12,), (6,), std_0=1., n_features=64, use_cuda=use_cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from air_hockey_challenge.framework.evaluate_agent import generate_init_states\n",
    "\n",
    "\n",
    "def generate_init_joint_states(mdp ,env: str = \"3dof-hit\"):\n",
    "\n",
    "    # create initial state\n",
    "    obs = generate_init_states(env=env, n_episodes=1, n_parallel_cores=1)\n",
    "    obs = obs[0].flatten()\n",
    "\n",
    "    # get joint position limits\n",
    "    min_pos_limits = mdp.env_info['robot']['joint_pos_limit'][0, :]\n",
    "    max_pos_limits = mdp.env_info['robot']['joint_pos_limit'][1, :]\n",
    "\n",
    "    # generate random joint positions\n",
    "    random_joint_pos = np.random.uniform(low=min_pos_limits, high=max_pos_limits, size=(3,))\n",
    "\n",
    "    # save random joint positions in the initial state\n",
    "    obs[mdp.env_info['joint_pos_ids']] = random_joint_pos\n",
    "\n",
    "    obs = obs.reshape((1,-1))\n",
    "\n",
    "    return obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = \"3dof-hit\"\n",
    "\n",
    "mdp = AirHockeyChallengeWrapper(env)\n",
    "mdp.reset()\n",
    "\n",
    "agent = build_agent_hitting(mdp.env_info)\n",
    "core = ChallengeCore(agent, mdp)\n",
    "\n",
    "n_expert_traj = 1000\n",
    "dataset_path = f'dataset/hit_{n_expert_traj}.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataset = []\n",
    "success_traj = 0\n",
    "for i in tqdm(range(n_expert_traj)):\n",
    "    # initial_state = generate_init_joint_states(mdp, env=env)\n",
    "    # new_traj = core.evaluate(initial_states=initial_state)\n",
    "    new_traj = core.evaluate(n_episodes=1)\n",
    "    _, _, _, _, absorbed, _ = parse_dataset(new_traj)\n",
    "    if absorbed[-1]: # we do not want examples where the \"expert\" did not score\n",
    "        dataset += new_traj\n",
    "        success_traj += 1\n",
    "print(f\"Dataset size: {len(dataset)} from {success_traj} trajectories\")\n",
    "\n",
    "with open(dataset_path, 'wb') as f:\n",
    "    pickle.dump(dataset, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from air_hockey_challenge.utils.replay_dataset import replay_dataset\n",
    "replay_dataset(env, dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_bc(policy, dataset, n_epochs, batch_size, n_batches_per_epoch, lr=0.0005):\n",
    "    optimizer = torch.optim.Adam(policy.parameters(), lr=lr)\n",
    "    \n",
    "    pbar = tqdm(range(n_epochs), desc='Epoch')\n",
    "    logs = dict(loss=[], epoch=[])\n",
    "    for iter in pbar:\n",
    "        avg = []\n",
    "        for i in range(n_batches_per_epoch):\n",
    "            states, actions, rewards, next_states, absorbing, last  = select_random_samples(dataset, batch_size, parse=True)\n",
    "            states_t = torch.from_numpy(states).to(device)\n",
    "            expert_actions_t = torch.from_numpy(actions.reshape((batch_size,-1))).to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss = -policy.log_prob_t(states_t, expert_actions_t).mean()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            avg.append(loss.item())\n",
    "        \n",
    "        avg_loss = np.mean(avg)\n",
    "        pbar.set_postfix({'loss': avg_loss})\n",
    "        logs['loss'].append(avg_loss)\n",
    "        logs['epoch'].append(iter)\n",
    "    return logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataset = np.load(f\"dataset/hit_{n_expert_traj}.pkl\", allow_pickle=True)\n",
    "\n",
    "logs = train_bc(policy, dataset, 100, 100, 100)\n",
    "policy.save('dataset/hit_500_policy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(logs['epoch'], logs['loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mushroom_rl.algorithms.actor_critic import PPO\n",
    "\n",
    "class PPOAgent(PPO): # Just to fix the action shape\n",
    "    def draw_action(self, state):\n",
    "        return super().draw_action(state).reshape(2,3)\n",
    "    \n",
    "    def fit(self, dataset, **info):\n",
    "        for i in range(len(dataset)):\n",
    "            dataset[i] = list(dataset[i])\n",
    "            dataset[i][1] = dataset[i][1].flatten()\n",
    "        return super().fit(dataset, **info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                             \r"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "class BCAgent(AgentBase):\n",
    "    def __init__(self, env_info, policy, **kwargs):\n",
    "        super().__init__(env_info, **kwargs)\n",
    "        self.policy = policy\n",
    "\n",
    "    def reset(self):\n",
    "        pass\n",
    "\n",
    "    def draw_action(self, observation):\n",
    "        return self.policy.draw_action(observation).reshape(2,3)\n",
    "    \n",
    "critic_params = dict(network=Network,\n",
    "                        optimizer={'class': torch.optim.Adam,\n",
    "                                'params': {}},\n",
    "                        loss=torch.nn.functional.mse_loss,\n",
    "                        n_features=64,\n",
    "                        batch_size=64,\n",
    "                        input_shape=(12,),\n",
    "                        output_shape=(1,), \n",
    "                        use_cuda = False)\n",
    "\n",
    "alg_params = dict(actor_optimizer={'class': torch.optim.Adam,\n",
    "                                    'params': {}},\n",
    "                    critic_params=critic_params,\n",
    "                    n_epochs_policy = 1,\n",
    "                    batch_size = 5,\n",
    "                    eps_ppo = 0.01,\n",
    "                    lam = 0.95,\n",
    "                 )\n",
    "\n",
    "policy_params = dict(\n",
    "    std_0=1.,\n",
    "    n_features=64,\n",
    "    use_cuda=False\n",
    ")\n",
    "\n",
    "policy = GaussianTorchPolicy(Network,\n",
    "                                (12,),\n",
    "                                (6,),\n",
    "                                **policy_params)\n",
    "# policy.load(\"dataset/policy\")\n",
    "\n",
    "agent = PPOAgent(mdp.info, policy, **alg_params)\n",
    "agent = agent.load('dataset/PPO/epoch_14.pkl')\n",
    "\n",
    "env = \"3dof-hit\"\n",
    "\n",
    "mdp = AirHockeyChallengeWrapper(env)\n",
    "mdp.reset()\n",
    "\n",
    "# agent = BCAgent(mdp.env_info, policy)\n",
    "core = ChallengeCore(agent, mdp)\n",
    "\n",
    "eval = core.evaluate(n_episodes=5, render=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def build_agent(env_info, **kwargs):\n",
    "    \"\"\"\n",
    "    Function where an Agent that controls the environments should be returned.\n",
    "    The Agent should inherit from the mushroom_rl Agent base env.\n",
    "\n",
    "    :param env_info: The environment information\n",
    "    :return: Either Agent ot Policy\n",
    "    \"\"\"\n",
    "    if \"hit\" in env_info[\"env_name\"]:\n",
    "        return BCAgent(mdp.env_info, policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================================\n",
      "Environment:        3dof-hit\n",
      "Number of Episodes: 50\n",
      "Success:            0.1000\n",
      "Penalty:            202.5\n",
      "Number of Violations: \n",
      "  joint_vel_constr  50\n",
      "  ee_constr         32\n",
      "  Jerk              50\n",
      "  joint_pos_constr  3\n",
      "  Computation Time  1\n",
      "  Total             136\n",
      "-------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from air_hockey_challenge.framework.evaluate_agent import evaluate\n",
    "\n",
    "eval = evaluate(build_agent, 'logs/bc_agent', env_list=[env], n_episodes=50, render=False, quiet=False, n_cores=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "challenge",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
