{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cuda: True\n"
     ]
    }
   ],
   "source": [
    "from air_hockey_challenge.framework.air_hockey_challenge_wrapper import AirHockeyChallengeWrapper\n",
    "from air_hockey_challenge.framework.challenge_core import ChallengeCore\n",
    "from air_hockey_challenge.framework.agent_base import AgentBase\n",
    "from examples.control.hitting_agent import build_agent, HittingAgent\n",
    "from examples.control.hitting_agent_wait import HittingAgentWait\n",
    "\n",
    "from mushroom_rl.utils.dataset import parse_dataset, select_random_samples\n",
    "from mushroom_rl.policy import GaussianTorchPolicy\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = 'cuda' if use_cuda else 'cpu'\n",
    "print(f\"Cuda: {use_cuda}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "\n",
    "class DaggerDataset(Dataset):\n",
    "    def __init__(self, states, expert_actions):\n",
    "        self.states = torch.from_numpy(states).to(device)\n",
    "        self.expert_actions = torch.from_numpy(expert_actions).to(device).reshape(-1,6)\n",
    "\n",
    "    def add(self, state, expert_action):\n",
    "        self.states = torch.cat((self.states, torch.from_numpy(state).to(device).reshape(-1,12)), dim = 0)\n",
    "        self.expert_actions = torch.cat((self.expert_actions, torch.from_numpy(expert_action).to(device).reshape(-1,6)), dim=0)\n",
    "        \n",
    "    def save(self, path):\n",
    "        with open(path, 'wb') as f:\n",
    "            pickle.dump((self.states.to(\"cpu\"), self.expert_actions.to(\"cpu\")), f)\n",
    "\n",
    "    def load(self, path):\n",
    "        states, expert_actions = np.load(path, allow_pickle=True)\n",
    "        self.states = states.to(device)\n",
    "        self.expert_actions = expert_actions.to(device)\n",
    "\n",
    "    def __len__(self):\n",
    "        \n",
    "        return len(self.states)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.states[index], self.expert_actions[index]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self, input_shape, output_shape, n_features, **kwargs):\n",
    "        super(Network, self).__init__()\n",
    "\n",
    "        self._h1 = nn.Linear(input_shape[0], n_features)\n",
    "        self._h2 = nn.Linear(n_features, n_features)\n",
    "        self._h3 = nn.Linear(n_features, output_shape[0])\n",
    "\n",
    "        nn.init.xavier_uniform_(self._h1.weight,\n",
    "                                gain=nn.init.calculate_gain('tanh'))\n",
    "        nn.init.xavier_uniform_(self._h2.weight,\n",
    "                                gain=nn.init.calculate_gain('tanh'))\n",
    "        nn.init.xavier_uniform_(self._h3.weight,\n",
    "                                gain=nn.init.calculate_gain('linear'))\n",
    "\n",
    "    def forward(self, obs, **kwargs):\n",
    "        features1 = torch.tanh(self._h1(torch.squeeze(obs, 1).float()))\n",
    "        features2 = torch.tanh(self._h2(features1))\n",
    "        return self._h3(features2)\n",
    "\n",
    "\n",
    "class BCAgent(AgentBase):\n",
    "    def __init__(self, env_info, policy, **kwargs):\n",
    "        super().__init__(env_info, **kwargs)\n",
    "        self.policy = policy\n",
    "\n",
    "    def reset(self):\n",
    "        pass\n",
    "\n",
    "    def draw_action(self, observation):\n",
    "        return self.policy.draw_action(observation).reshape(2,3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from air_hockey_challenge.framework.evaluate_agent import PENALTY_POINTS\n",
    "from air_hockey_challenge.utils.kinematics import forward_kinematics\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def custom_reward_function(base_env, state, action, next_state, absorbing):\n",
    "    reward = -0.1 # small value to disincentivate long runs: faster is better\n",
    "\n",
    "    q = state[base_env.env_info['joint_pos_ids']]\n",
    "    dq = state[base_env.env_info['joint_vel_ids']]\n",
    "\n",
    "    # Get a dictionary of the constraint functions {\"constraint_name\": ndarray}\n",
    "    c = base_env.env_info['constraints'].fun(q, dq)\n",
    "\n",
    "    # for k,v in c.items():\n",
    "    #     reward -= PENALTY_POINTS[k] * np.sum(v > 0)\n",
    "    # mybe not ideal: penalty is flat outside, does not know how to improve --> use violation instead of points?\n",
    "\n",
    "    for k,val in c.items():\n",
    "        for v in val:\n",
    "            reward -= v if v > 0 else 0\n",
    "\n",
    "    # TODO reward for good actions --> score\n",
    "    # for now --> try to go to the puck\n",
    "    \n",
    "    ee_pos = forward_kinematics(base_env.env_info['robot']['robot_model'], base_env.env_info['robot']['robot_data'], q)[0]\n",
    "    puck_pos = state[base_env.env_info['puck_pos_ids']]\n",
    "    puck_vel = np.linalg.norm(state[base_env.env_info['puck_vel_ids']])\n",
    "    reward += -np.linalg.norm(ee_pos - puck_pos) if puck_vel < 0.1 else puck_vel\n",
    "\n",
    "    return reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mushroom_rl.utils.dataset import compute_metrics\n",
    "\n",
    "\n",
    "def train_dagger_agent(learner_policy, expert_policy, mdp, dataset, num_iterations=10, batch_size=100, lr=0.001, n_steps=100, num_sub_epochs = 50, n_episode_eval=5):\n",
    "    # Create data loader\n",
    "    data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # Set up the optimizer and loss function\n",
    "    optimizer = optim.Adam(learner_policy.parameters(), lr=lr)\n",
    "\n",
    "    # Train the initial policy on collected data\n",
    "    for epoch in tqdm(range(num_iterations)):\n",
    "        for sub_epoch in range(num_sub_epochs):\n",
    "            # print(len(data_loader))\n",
    "            for i in range(10):\n",
    "                for states, expert_actions in data_loader:\n",
    "\n",
    "                    optimizer.zero_grad()\n",
    "\n",
    "                    states = states.to(device)\n",
    "                    expert_actions = expert_actions.to(device)\n",
    "\n",
    "                    loss = -learner_policy.log_prob_t(states, expert_actions).mean()\n",
    "\n",
    "                    # Update the learner policy\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "            # Evaluate the current learner policy\n",
    "            learner_core = ChallengeCore(BCAgent(mdp.env_info, learner_policy), mdp)\n",
    "            learner_trajectory = learner_core.evaluate(n_steps=n_steps)\n",
    "            states, action, reward, next_state, absorbing, last = parse_dataset(learner_trajectory)\n",
    "\n",
    "            i = 0\n",
    "            for state in states:\n",
    "                expert_policy.reset()\n",
    "                expert_action = expert_policy.draw_action(state).astype(np.float32)\n",
    "                if not expert_policy.optimization_failed:\n",
    "                    dataset.add(state, expert_action.flatten())\n",
    "                    i += 1\n",
    "            # print(f\"added {i} out of {len(states)}\")\n",
    "        \n",
    "        core = ChallengeCore(BCAgent(mdp.env_info, learner_policy), mdp)\n",
    "        eval_dataset = core.evaluate(n_episodes=n_episode_eval, render=False)\n",
    "        metrics = compute_metrics(eval_dataset)\n",
    "        print(\"Epoch:\",epoch+1,\"metrics:\",metrics)\n",
    "        learner_policy.save(f\"dataset/Dagger/policy_{epoch+1}.pkl\")\n",
    "        dataset.save(f\"dataset/Dagger/dataset_{epoch+1}.pkl\")\n",
    "\n",
    "    return learner_policy\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 1. defining BCAgent and expert agent to be trained in dagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "env = \"3dof-hit\"\n",
    "\n",
    "mdp = AirHockeyChallengeWrapper(env, custom_reward_function=custom_reward_function)\n",
    "mdp.reset()\n",
    "\n",
    "# policy can only output 1D actions (6,) ... they need to be recast in (2,3) shape later on\n",
    "policy = GaussianTorchPolicy(Network, (12,), (6,), std_0=1., n_features=64, use_cuda=use_cuda)\n",
    "\n",
    "# policy = policy.load('dataset/hit_500_policy')\n",
    "\n",
    "bc_agent = BCAgent(mdp.env_info, policy)\n",
    "# dataset = MushroomRLTrajectoryDataset(mdp, bc_agent, n_episodes=10)\n",
    "\n",
    "states, actions, reward, next_state, absorbing, last = parse_dataset(np.load(f\"dataset/hit_500.pkl\", allow_pickle=True))\n",
    "\n",
    "dataset = DaggerDataset(states, actions)\n",
    "\n",
    "expert_agent = HittingAgentWait(mdp.env_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# dagger_agent = train_dagger_agent(policy, expert_agent, mdp, dataset, num_iterations=1, batch_size=64, lr=0.001)\n",
    "dagger_agent = train_dagger_agent(policy, expert_agent, mdp, dataset, num_iterations=20)\n",
    "# dagger_agent = train_dagger_agent(policy, expert_agent, mdp, dataset, num_sub_epochs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================================\n",
      "Environment:        3dof-hit\n",
      "Number of Episodes: 50\n",
      "Success:            0.0000\n",
      "Penalty:            348\n",
      "Number of Violations: \n",
      "  joint_pos_constr  49\n",
      "  joint_vel_constr  50\n",
      "  ee_constr         50\n",
      "  Jerk              50\n",
      "  Total             199\n",
      "-------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from air_hockey_challenge.framework.evaluate_agent import evaluate\n",
    "from mushroom_rl.policy import GaussianTorchPolicy\n",
    "\n",
    "\n",
    "def build_agent(env_info, **kwargs):\n",
    "    \"\"\"\n",
    "    Function where an Agent that controls the environments should be returned.\n",
    "    The Agent should inherit from the mushroom_rl Agent base env.\n",
    "\n",
    "    :param env_info: The environment information\n",
    "    :return: Either Agent ot Policy\n",
    "    \"\"\"\n",
    "    if \"hit\" in env_info[\"env_name\"]:\n",
    "        # policy = GaussianTorchPolicy(Network, (12,), (6,), std_0=1., n_features=64, use_cuda=use_cuda)\n",
    "\n",
    "        # policy = policy.load('dataset/hit_500_policy')\n",
    "        dagger_ag = BCAgent(mdp.env_info, dagger_agent)\n",
    "        # return agent\n",
    "        return dagger_ag\n",
    "\n",
    "eval = evaluate(build_agent, 'logs/dagger_agent', env_list=[\"3dof-hit\"], n_episodes=50, render=False, quiet=False, n_cores=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================================\n",
      "Environment:        3dof-hit\n",
      "Number of Episodes: 5\n",
      "Success:            0.0000\n",
      "Penalty:            35\n",
      "Number of Violations: \n",
      "  joint_pos_constr  5\n",
      "  joint_vel_constr  5\n",
      "  ee_constr         5\n",
      "  Jerk              5\n",
      "  Total             20\n",
      "-------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "eval = evaluate(build_agent, 'logs/dagger_agent', env_list=[\"3dof-hit\"], n_episodes=5, render=True, quiet=False, n_cores=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "127416\n",
      "50823\n"
     ]
    }
   ],
   "source": [
    "print(len(dataset))\n",
    "\n",
    "\n",
    "states, actions, reward, next_state, absorbing, last = parse_dataset(np.load(f\"dataset/hit_500.pkl\", allow_pickle=True))\n",
    "\n",
    "print(len(states))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: unspecified launch failure\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mdataset/dagger\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mwb\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m f:\n\u001b[0;32m----> 2\u001b[0m     pickle\u001b[39m.\u001b[39;49mdump(dataset, f)\n",
      "File \u001b[0;32m~/Documents/MIT/6.8200_Sensorimotor_learning/final project/air_hockey_challenge/challenge/lib/python3.8/site-packages/torch/storage.py:747\u001b[0m, in \u001b[0;36mTypedStorage.__reduce__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    745\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__reduce__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    746\u001b[0m     b \u001b[39m=\u001b[39m io\u001b[39m.\u001b[39mBytesIO()\n\u001b[0;32m--> 747\u001b[0m     torch\u001b[39m.\u001b[39;49msave(\u001b[39mself\u001b[39;49m, b, _use_new_zipfile_serialization\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m    748\u001b[0m     \u001b[39mreturn\u001b[39;00m (_load_from_bytes, (b\u001b[39m.\u001b[39mgetvalue(),))\n",
      "File \u001b[0;32m~/Documents/MIT/6.8200_Sensorimotor_learning/final project/air_hockey_challenge/challenge/lib/python3.8/site-packages/torch/serialization.py:445\u001b[0m, in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization)\u001b[0m\n\u001b[1;32m    443\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    444\u001b[0m     \u001b[39mwith\u001b[39;00m _open_file_like(f, \u001b[39m'\u001b[39m\u001b[39mwb\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m opened_file:\n\u001b[0;32m--> 445\u001b[0m         _legacy_save(obj, opened_file, pickle_module, pickle_protocol)\n",
      "File \u001b[0;32m~/Documents/MIT/6.8200_Sensorimotor_learning/final project/air_hockey_challenge/challenge/lib/python3.8/site-packages/torch/serialization.py:589\u001b[0m, in \u001b[0;36m_legacy_save\u001b[0;34m(obj, f, pickle_module, pickle_protocol)\u001b[0m\n\u001b[1;32m    587\u001b[0m \u001b[39mfor\u001b[39;00m key \u001b[39min\u001b[39;00m serialized_storage_keys:\n\u001b[1;32m    588\u001b[0m     storage, dtype \u001b[39m=\u001b[39m serialized_storages[key]\n\u001b[0;32m--> 589\u001b[0m     storage\u001b[39m.\u001b[39;49m_write_file(f, _should_read_directly(f), \u001b[39mTrue\u001b[39;49;00m, torch\u001b[39m.\u001b[39;49m_utils\u001b[39m.\u001b[39;49m_element_size(dtype))\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: unspecified launch failure\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "with open(\"dataset/dagger\", 'wb') as f:\n",
    "    pickle.dump(dataset, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "core = ChallengeCore(expert_agent, mdp)\n",
    "eval_dataset = core.evaluate(n_episodes=5, render=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "595\n"
     ]
    }
   ],
   "source": [
    "print(len(eval_dataset))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# testing expert agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_loader = DataLoader(dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "for i, data in enumerate(data_loader):\n",
    "    states = data['state'].float()\n",
    "    for state in states:\n",
    "        expert_agent.reset()\n",
    "        state_np = state.cpu().numpy()\n",
    "        expert_action = expert_agent.draw_action(state_np)\n",
    "        print(f\"expert_action: {expert_action}\")\n",
    "        print(f\"optimization_failed: {expert_agent.optimization_failed}\")\n",
    "    # expert_actions = expert_agent.draw_action(states).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
